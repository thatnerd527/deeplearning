{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3b59d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-28 11:27:18.627331: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ac467d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run createDerivation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3434900",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1764329256.261104    9607 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3282 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "def image_to_vector(image: np.ndarray, final_length: int = 2048, max_features_to_detect: int = 5000) -> np.ndarray | None:\n",
    "    \"\"\"\n",
    "    Converts an image into a fixed-length 1D feature vector using ORB.\n",
    "\n",
    "    The function detects ORB features, selects the strongest ones, and flattens\n",
    "    their descriptors into a single vector.\n",
    "\n",
    "    Args:\n",
    "        image (np.ndarray): The input image (can be color or grayscale).\n",
    "        final_length (int): The desired length of the output vector.\n",
    "                              MUST be a multiple of 32. Defaults to 1600 (50 features * 32).\n",
    "        max_features_to_detect (int): The maximum number of features for ORB to detect initially.\n",
    "                                      Should be significantly larger than (final_length / 32).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray | None: A 1D NumPy array of dtype uint8 with the specified length,\n",
    "                          or None if not enough features could be detected in the image.\n",
    "    \"\"\"\n",
    "    # 1. Validate inputs\n",
    "    if image is None:\n",
    "        print(\"Error: Input image is None.\")\n",
    "        return None\n",
    "\n",
    "    if final_length % 32 != 0:\n",
    "        raise ValueError(\"Error: final_length must be a multiple of 32.\")\n",
    "\n",
    "    # Calculate the exact number of features we need to sample\n",
    "    num_features_to_sample = final_length // 32\n",
    "\n",
    "    # 2. Ensure the image is grayscale\n",
    "    if len(image.shape) == 3 and image.shape[2] == 3:\n",
    "        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray_image = image # Assume it's already grayscale\n",
    "\n",
    "    # 3. Initialize ORB detector\n",
    "    orb = cv2.ORB_create(nfeatures=max_features_to_detect)\n",
    "\n",
    "    # 4. Detect keypoints and compute descriptors\n",
    "    kps, des = orb.detectAndCompute(gray_image, None)\n",
    "\n",
    "    # 5. Check if enough descriptors were found\n",
    "    if des is None or len(des) < num_features_to_sample:\n",
    "        print(f\"Warning: Found only {0 if des is None else len(des)} features, but need {num_features_to_sample}. Cannot create a vector of length {final_length}.\")\n",
    "        return None\n",
    "\n",
    "    # 6. Sort features by response score (strongest first)\n",
    "    kp_des_pairs = sorted(zip(kps, des), key=lambda x: x[0].response, reverse=True)\n",
    "\n",
    "    # 7. Select the top N features\n",
    "    top_pairs = kp_des_pairs[:num_features_to_sample]\n",
    "    \n",
    "    # We only need the descriptors from the top pairs\n",
    "    _, sampled_des = zip(*top_pairs)\n",
    "    \n",
    "    # 8. Flatten the list of descriptors into a single 1D vector\n",
    "    feature_vector = np.array(sampled_des).flatten()\n",
    "\n",
    "    return feature_vector\n",
    "\n",
    "model = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
    "\n",
    "def image_to_vector2(path: str):\n",
    "    img_path = path\n",
    "    # The target_size must match the size the model was trained on (224x224 for ResNet50)\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array_expanded = np.expand_dims(img_array, axis=0)\n",
    "    img_preprocessed = preprocess_input(img_array_expanded)\n",
    "\n",
    "    # 3. Use the model to predict (extract) features\n",
    "    features = model.predict(img_preprocessed, verbose=0)\n",
    "    return features[0]\n",
    "def keep_all(path): return True\n",
    "def path_2_features(path: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Reads an image from the given path and converts it to a string representation of its feature vector.\n",
    "    in comma-separated format.\n",
    "    Args:\n",
    "        path (str): The file path to the image.\n",
    "    Returns:\n",
    "        str | None: A comma-separated string of the feature vector, or None if conversion fails\n",
    "    \"\"\" \n",
    "    try:\n",
    "        feature_vector = image_to_vector2(path)\n",
    "        if feature_vector is None:\n",
    "            return \"0\"\n",
    "        return ','.join(map(str, feature_vector.tolist())).encode('utf-8')\n",
    "    except Exception as e:\n",
    "        print(f\"Exception occurred while processing image at path {path}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fa2e887",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_content = \"\"\n",
    "with open(\"final_variation.txt\", 'r') as file:\n",
    "    file_content = file.read()\n",
    "\n",
    "filtered2_path = file_content.strip()\n",
    "filtered2_df = pd.read_csv(filtered2_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4be3c211",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-28 11:28:32.610495: I external/local_xla/xla/service/service.cc:163] XLA service 0x7f3fc8006290 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-11-28 11:28:32.610526: I external/local_xla/xla/service/service.cc:171]   StreamExecutor device (0): NVIDIA GeForce RTX 4060, Compute Capability 8.9\n",
      "2025-11-28 11:28:32.785570: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-11-28 11:28:33.336342: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7f404b3d6a20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7f404b3d6a20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1764329314.973973   33020 device_compiler.h:196] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 3826 rows to /home/fadhlan/Normal2/DeepLearningRepo/steps/variations/var_b7376b7ae18d395b/b7376b7ae18d395b.csv (variation b7376b7ae18d395b)\n",
      "Created variation CSV with features: /home/fadhlan/Normal2/DeepLearningRepo/steps/variations/var_b7376b7ae18d395b/b7376b7ae18d395b.csv\n"
     ]
    }
   ],
   "source": [
    "processed = create_dataset_variation(filtered2_df, keep_all, path_2_features, variation_tag=\"orb_features_2048\")\n",
    "print(\"Created variation CSV with features:\", processed)\n",
    "extracted_features_path = open(\"final_features.txt\", \"w\")\n",
    "extracted_features_path.write(processed)\n",
    "extracted_features_path.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
