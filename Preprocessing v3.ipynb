{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f09c959",
   "metadata": {},
   "source": [
    "# Data Processing V3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db46646",
   "metadata": {},
   "source": [
    "### Build and Run Dataset Downloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ec95c3",
   "metadata": {},
   "source": [
    "### This preprocessing pipeline uses createDerivation.py\n",
    "\n",
    "1. Build the \"paralells\" and run the program to download and convert the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "756587b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[1m\u001b[33mwarning\u001b[0m\u001b[0m\u001b[1m: unused import: `csv::StringRecord`\u001b[0m\n",
      "\u001b[0m \u001b[0m\u001b[0m\u001b[1m\u001b[38;5;12m--> \u001b[0m\u001b[0msrc/main.rs:2:5\u001b[0m\n",
      "\u001b[0m  \u001b[0m\u001b[0m\u001b[1m\u001b[38;5;12m|\u001b[0m\n",
      "\u001b[0m\u001b[1m\u001b[38;5;12m2\u001b[0m\u001b[0m \u001b[0m\u001b[0m\u001b[1m\u001b[38;5;12m|\u001b[0m\u001b[0m \u001b[0m\u001b[0muse csv::StringRecord;\u001b[0m\n",
      "\u001b[0m  \u001b[0m\u001b[0m\u001b[1m\u001b[38;5;12m|\u001b[0m\u001b[0m     \u001b[0m\u001b[0m\u001b[1m\u001b[33m^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "\u001b[0m  \u001b[0m\u001b[0m\u001b[1m\u001b[38;5;12m|\u001b[0m\n",
      "\u001b[0m  \u001b[0m\u001b[0m\u001b[1m\u001b[38;5;12m= \u001b[0m\u001b[0m\u001b[1mnote\u001b[0m\u001b[0m: `#[warn(unused_imports)]` on by default\u001b[0m\n",
      "\n",
      "\u001b[0m\u001b[1m\u001b[33mwarning\u001b[0m\u001b[0m\u001b[1m: unused import: `http::Request`\u001b[0m\n",
      "\u001b[0m \u001b[0m\u001b[0m\u001b[1m\u001b[38;5;12m--> \u001b[0m\u001b[0msrc/main.rs:3:5\u001b[0m\n",
      "\u001b[0m  \u001b[0m\u001b[0m\u001b[1m\u001b[38;5;12m|\u001b[0m\n",
      "\u001b[0m\u001b[1m\u001b[38;5;12m3\u001b[0m\u001b[0m \u001b[0m\u001b[0m\u001b[1m\u001b[38;5;12m|\u001b[0m\u001b[0m \u001b[0m\u001b[0muse http::Request;\u001b[0m\n",
      "\u001b[0m  \u001b[0m\u001b[0m\u001b[1m\u001b[38;5;12m|\u001b[0m\u001b[0m     \u001b[0m\u001b[0m\u001b[1m\u001b[33m^^^^^^^^^^^^^\u001b[0m\n",
      "\n",
      "\u001b[0m\u001b[1m\u001b[33mwarning\u001b[0m\u001b[0m\u001b[1m: unused import: `self`\u001b[0m\n",
      "\u001b[0m \u001b[0m\u001b[0m\u001b[1m\u001b[38;5;12m--> \u001b[0m\u001b[0msrc/main.rs:6:25\u001b[0m\n",
      "\u001b[0m  \u001b[0m\u001b[0m\u001b[1m\u001b[38;5;12m|\u001b[0m\n",
      "\u001b[0m\u001b[1m\u001b[38;5;12m6\u001b[0m\u001b[0m \u001b[0m\u001b[0m\u001b[1m\u001b[38;5;12m|\u001b[0m\u001b[0m \u001b[0m\u001b[0muse reqwest::blocking::{self, Client};\u001b[0m\n",
      "\u001b[0m  \u001b[0m\u001b[0m\u001b[1m\u001b[38;5;12m|\u001b[0m\u001b[0m                         \u001b[0m\u001b[0m\u001b[1m\u001b[33m^^^^\u001b[0m\n",
      "\n",
      "\u001b[0m\u001b[1m\u001b[33mwarning\u001b[0m\u001b[0m\u001b[1m: unused imports: `DirEntry`, `FileType`, `self`, and `time::Duration`\u001b[0m\n",
      "\u001b[0m  \u001b[0m\u001b[0m\u001b[1m\u001b[38;5;12m--> \u001b[0m\u001b[0msrc/main.rs:9:16\u001b[0m\n",
      "\u001b[0m   \u001b[0m\u001b[0m\u001b[1m\u001b[38;5;12m|\u001b[0m\n",
      "\u001b[0m \u001b[0m\u001b[0m\u001b[1m\u001b[38;5;12m9\u001b[0m\u001b[0m \u001b[0m\u001b[0m\u001b[1m\u001b[38;5;12m|\u001b[0m\u001b[0m \u001b[0m\u001b[0m    fs::{self, DirEntry, File, FileType},\u001b[0m\n",
      "\u001b[0m   \u001b[0m\u001b[0m\u001b[1m\u001b[38;5;12m|\u001b[0m\u001b[0m                \u001b[0m\u001b[0m\u001b[1m\u001b[33m^^^^^^^^\u001b[0m\u001b[0m        \u001b[0m\u001b[0m\u001b[1m\u001b[33m^^^^^^^^\u001b[0m\n",
      "\u001b[0m\u001b[1m\u001b[38;5;12m10\u001b[0m\u001b[0m \u001b[0m\u001b[0m\u001b[1m\u001b[38;5;12m|\u001b[0m\u001b[0m \u001b[0m\u001b[0m    io::{Cursor, Read, Write},\u001b[0m\n",
      "\u001b[0m\u001b[1m\u001b[38;5;12m11\u001b[0m\u001b[0m \u001b[0m\u001b[0m\u001b[1m\u001b[38;5;12m|\u001b[0m\u001b[0m \u001b[0m\u001b[0m    path::{self, Path, PathBuf},\u001b[0m\n",
      "\u001b[0m   \u001b[0m\u001b[0m\u001b[1m\u001b[38;5;12m|\u001b[0m\u001b[0m            \u001b[0m\u001b[0m\u001b[1m\u001b[33m^^^^\u001b[0m\n",
      "\u001b[0m\u001b[1m\u001b[38;5;12m12\u001b[0m\u001b[0m \u001b[0m\u001b[0m\u001b[1m\u001b[38;5;12m|\u001b[0m\u001b[0m \u001b[0m\u001b[0m    time::Duration,\u001b[0m\n",
      "\u001b[0m   \u001b[0m\u001b[0m\u001b[1m\u001b[38;5;12m|\u001b[0m\u001b[0m     \u001b[0m\u001b[0m\u001b[1m\u001b[33m^^^^^^^^^^^^^^\u001b[0m\n",
      "\n",
      "\u001b[0m\u001b[1m\u001b[33mwarning\u001b[0m\u001b[0m\u001b[1m: unused variable: `baseid`\u001b[0m\n",
      "\u001b[0m   \u001b[0m\u001b[0m\u001b[1m\u001b[38;5;12m--> \u001b[0m\u001b[0msrc/main.rs:134:37\u001b[0m\n",
      "\u001b[0m    \u001b[0m\u001b[0m\u001b[1m\u001b[38;5;12m|\u001b[0m\n",
      "\u001b[0m\u001b[1m\u001b[38;5;12m134\u001b[0m\u001b[0m \u001b[0m\u001b[0m\u001b[1m\u001b[38;5;12m|\u001b[0m\u001b[0m \u001b[0m\u001b[0m            InputFile::Real { path, baseid } => Some(vec![path.clone()]),\u001b[0m\n",
      "\u001b[0m    \u001b[0m\u001b[0m\u001b[1m\u001b[38;5;12m|\u001b[0m\u001b[0m                                     \u001b[0m\u001b[0m\u001b[1m\u001b[33m^^^^^^\u001b[0m\u001b[0m \u001b[0m\u001b[0m\u001b[1m\u001b[33mhelp: try ignoring the field: `baseid: _`\u001b[0m\n",
      "\u001b[0m    \u001b[0m\u001b[0m\u001b[1m\u001b[38;5;12m|\u001b[0m\n",
      "\u001b[0m    \u001b[0m\u001b[0m\u001b[1m\u001b[38;5;12m= \u001b[0m\u001b[0m\u001b[1mnote\u001b[0m\u001b[0m: `#[warn(unused_variables)]` on by default\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[33mwarning\u001b[0m\u001b[1m:\u001b[0m `paralells` (bin \"paralells\") generated 5 warnings (run `cargo fix --bin \"paralells\"` to apply 4 suggestions)\n",
      "\u001b[1m\u001b[32m    Finished\u001b[0m `release` profile [optimized] target(s) in 0.35s\n"
     ]
    }
   ],
   "source": [
    "!cd sub/paralells ; cargo build --release ; ./target/release/paralells --inputfolder \"input (new data)\" --outputfile result.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba8fc1f",
   "metadata": {},
   "source": [
    "Paralells is a high-speed Rust-based processing tool that can be used to parallelize tasks such as downloading and converting datasets.\n",
    "More specifically, it downloads all images from CSV and converts them in parallel to PNG format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef102f6",
   "metadata": {},
   "source": [
    "2. Use YOLOv11-seg to segment the dataset by apples (in place of avocados), and bananas, and crop the images to only contain the avocados and bananas.\n",
    "3. Apply unsharp mask to the cropped images to enhance the edges.\n",
    "4. Save the modifed images as a new derivation using createDerivation.py.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e295c4",
   "metadata": {},
   "source": [
    "### Use YOLOv11-seg to Segment and Crop Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae4ac7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading YOLO model: yolo11x-seg.pt ...\n",
      "PHASE 1: YOLO Detection and Crop Region Precomputation\n",
      "Scanning 5431 images for target objects: ['apple', 'avocado', 'banana']\n",
      "Found 3805 images with target detections. Skipped: 0\n",
      "Wrote 3826 rows to /home/fadhlan/Normal2/DeepLearningRepo/steps/variations/var_8d36ba17d2f23fc8/8d36ba17d2f23fc8.csv (variation 8d36ba17d2f23fc8)\n",
      "New variation CSV: /home/fadhlan/Normal2/DeepLearningRepo/steps/variations/var_8d36ba17d2f23fc8/8d36ba17d2f23fc8.csv\n"
     ]
    }
   ],
   "source": [
    "%run createDerivation.py\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ultralytics import YOLO\n",
    "\n",
    "INPUT_FOLDER = \"./sub/paralells/steps/convertedtemp/\" \n",
    "OUTPUT_FOLDER = \"processed_images\" \n",
    "TARGET_CLASSES = ['banana', 'apple', 'avocado'] \n",
    "FINAL_SIZE = (720, 720) \n",
    "MODEL_NAME = 'yolo11x-seg.pt' \n",
    "\n",
    "# read form paralells csv\n",
    "all_df = pd.read_csv(\"./sub/paralells/result.csv\")\n",
    "\n",
    "def keep_all(path): return True\n",
    "\n",
    "target_df = all_df\n",
    "\n",
    "# Ensure required globals exist\n",
    "assert 'MODEL_NAME' in globals(), \"MODEL_NAME not defined (from earlier YOLO cell).\"\n",
    "assert 'FINAL_SIZE' in globals(), \"FINAL_SIZE not defined (from earlier YOLO cell).\"\n",
    "assert 'TARGET_CLASSES' in globals(), \"TARGET_CLASSES not defined (from earlier YOLO cell).\"\n",
    "assert 'create_dataset_variation' in globals(), \"create_dataset_variation not available.\"\n",
    "\n",
    "print(f\"Loading YOLO model: {MODEL_NAME} ...\")\n",
    "\n",
    "try:\n",
    "    _yolo_model = YOLO(MODEL_NAME)\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Failed to load YOLO model '{MODEL_NAME}': {e}\")\n",
    "\n",
    "_target_classes = set(TARGET_CLASSES)\n",
    "\n",
    "def _best_square_crop_from_results(image, results, names):\n",
    "    img_h, img_w = image.shape[:2]\n",
    "    best_detection = None\n",
    "    highest_conf = -1.0\n",
    "\n",
    "    for result in results:\n",
    "        if result.boxes is None:\n",
    "            continue\n",
    "        for box in result.boxes:\n",
    "            class_id = int(box.cls[0])\n",
    "            class_name = names.get(class_id, str(class_id))\n",
    "            if class_name not in _target_classes:\n",
    "                continue\n",
    "            conf = float(box.conf[0])\n",
    "            if conf > highest_conf:\n",
    "                highest_conf = conf\n",
    "                x1, y1, x2, y2 = box.xyxy[0].cpu().numpy().tolist()\n",
    "                best_detection = (x1, y1, x2, y2)\n",
    "\n",
    "    if best_detection is None:\n",
    "        return None\n",
    "\n",
    "    x1, y1, x2, y2 = best_detection\n",
    "    cx = int((x1 + x2) / 2)\n",
    "    cy = int((y1 + y2) / 2)\n",
    "    dist_to_top = cy\n",
    "    dist_to_bottom = img_h - cy\n",
    "    dist_to_left = cx\n",
    "    dist_to_right = img_w - cx\n",
    "    half_size = int(min(dist_to_top, dist_to_bottom, dist_to_left, dist_to_right))\n",
    "    if half_size <= 0:\n",
    "        return None\n",
    "    return (\n",
    "        max(0, cx - half_size),\n",
    "        max(0, cy - half_size),\n",
    "        min(img_w, cx + half_size),\n",
    "        min(img_h, cy + half_size),\n",
    "    )\n",
    "\n",
    "print(\"PHASE 1: YOLO Detection and Crop Region Precomputation\")\n",
    "# Precompute crop regions for all valid images in base_df\n",
    "print(f\"Scanning {len(target_df)} images for target objects: {sorted(_target_classes)}\")\n",
    "_crop_index = {}\n",
    "_skipped = 0\n",
    "for i, row in target_df.iterrows():\n",
    "    p = row[\"file_path\"]\n",
    "    if not isinstance(p, str) or not os.path.isfile(p):\n",
    "        _skipped += 1\n",
    "        continue\n",
    "    img = cv2.imread(p)\n",
    "    if img is None:\n",
    "        _skipped += 1\n",
    "        continue\n",
    "    try:\n",
    "        results = _yolo_model(img, verbose=False)\n",
    "        crop_box = _best_square_crop_from_results(img, results, _yolo_model.names)\n",
    "        if crop_box is not None:\n",
    "            _crop_index[p] = crop_box\n",
    "    except Exception:\n",
    "        _skipped += 1\n",
    "\n",
    "print(f\"Found {len(_crop_index)} images with target detections. Skipped: {_skipped}\")\n",
    "\n",
    "def yolo_filter_fn(path: str) -> bool:\n",
    "    return path in _crop_index\n",
    "\n",
    "def yolo_crop_map_fn(path: str) -> bytes:\n",
    "    img = cv2.imread(path)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Could not read image: {path}\")\n",
    "    x1, y1, x2, y2 = _crop_index.get(path, (0, 0, img.shape[1], img.shape[0]))\n",
    "    cropped = img[y1:y2, x1:x2]\n",
    "    if cropped.size == 0:\n",
    "        cropped = img  # fallback to original if something went wrong\n",
    "    resized = cv2.resize(cropped, FINAL_SIZE, interpolation=cv2.INTER_AREA)\n",
    "    ok, buf = cv2.imencode(\".png\", resized)\n",
    "    if not ok:\n",
    "        raise RuntimeError(\"Failed to encode cropped image.\")\n",
    "    return buf.tobytes()\n",
    "\n",
    "variation_tag = f\"yolo_crop_{MODEL_NAME}_to_{FINAL_SIZE[0]}x{FINAL_SIZE[1]}\"\n",
    "step1_path = create_dataset_variation(target_df, yolo_filter_fn, yolo_crop_map_fn, variation_tag=variation_tag)\n",
    "print(\"New variation CSV:\", step1_path)\n",
    "f = open(\"step1.txt\", \"w\")\n",
    "f.write(step1_path)\n",
    "f.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
